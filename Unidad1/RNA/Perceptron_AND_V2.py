import random

# Inicializaci√≥n de pesos y bias con valores peque√±os aleatorios
w1 = random.uniform(-1, 1)  # Peso para x1
w2 = random.uniform(-1, 1)  # Peso para x2
bias = random.uniform(-1, 1)  # Bias (t√©rmino independiente)
learning_rate = 0.1  # Tasa de aprendizaje

# Datos de entrenamiento para la compuerta AND
training_data = [
    (0, 0, 0),
    (0, 1, 0),
    (1, 0, 0),
    (1, 1, 1),
]

# Funci√≥n de activaci√≥n: escal√≥n de Heaviside
def activation_function(z):
    return 1 if z >= 0 else 0

# Entrenamiento del perceptr√≥n
epochs = 10  # N√∫mero de iteraciones sobre el dataset

print(f"Pesos iniciales: w1={w1:.4f}, w2={w2:.4f}, bias={bias:.4f}")

for epoch in range(epochs):
    print(f"\n√âpoca {epoch + 1}")
    
    for x1, x2, y_real in training_data:
        # C√°lculo de la salida del perceptr√≥n
        z = (w1 * x1) + (w2 * x2) + bias
        y_predicho = activation_function(z)

        # Calcular error
        error = y_real - y_predicho

        # Actualizar pesos y bias
        w1 += learning_rate * error * x1
        w2 += learning_rate * error * x2
        bias += learning_rate * error

        # Mostrar valores intermedios
        print(f"Entrada: ({x1}, {x2}) | Salida esperada: {y_real} | Salida predicha: {y_predicho} | "
              f"Error: {error} | Nuevos pesos: w1={w1:.4f}, w2={w2:.4f}, bias={bias:.4f}")

# Evaluaci√≥n del modelo despu√©s del entrenamiento
print("\nüîπ Resultados finales tras entrenamiento:")
for x1, x2, y_real in training_data:
    z = (w1 * x1) + (w2 * x2) + bias
    y_predicho = activation_function(z)
    print(f"Entrada: ({x1}, {x2}) ‚Üí Salida esperada: {y_real} | Salida obtenida: {y_predicho}")
